import logging
import asyncio
import aiohttp
import os
import json #ë””ë²„ê¹…
from PIL import Image
from typing import Any
from verl.tools.base_tool import BaseTool
from verl.tools.schemas import ToolResponse

logger = logging.getLogger(__name__)

class SearchTool(BaseTool):
    def __init__(self, config: dict, tool_schema: Any):
        super().__init__(config, tool_schema)
        self.url = config.get("retrieval_service_url", "http://localhost:5002/search")
        self.timeout = config.get("timeout", 30)
        self.k = config.get("k", 5)
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ê³„ì‚° (search_tool.py ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ 4ë‹¨ê³„)
        self.project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
        # ì´ë¯¸ì§€ íŒŒì¼ì˜ ë¡œì»¬ ë£¨íŠ¸ ê²½ë¡œ (configì—ì„œ ìƒëŒ€ ê²½ë¡œë¡œ ì„¤ì • ê°€ëŠ¥)
        local_image_root = config.get("local_image_root", "./search_engnie/corpus/img")
        # ìƒëŒ€ ê²½ë¡œë©´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        if local_image_root.startswith("./"):
            self.local_image_root = os.path.join(self.project_root, local_image_root[2:])
        else:
            self.local_image_root = local_image_root
        logger.info(f"Initialized SearchTool with URL: {self.url}, local_image_root: {self.local_image_root}")

    async def execute(self, instance_id: str, parameters: dict[str, Any], **kwargs) -> ToolResponse:
        agent_data = kwargs.get('agent_data')
        query = parameters.get('query')

        # Extract sample_id from agent_data (original dataset id like "train_14")
        # This is REQUIRED - the search server needs the dataset id to find relevant documents
        sample_id = None
        if agent_data and hasattr(agent_data, 'sample_id'):
            sample_id = agent_data.sample_id

        if sample_id is None:
            error_msg = (
                "CRITICAL: sample_id is None! "
                "The data pipeline failed to pass the original dataset id (e.g., 'train_14'). "
                "Check: 1) ray_trainer.py uid assignment, 2) tool_agent_loop.py kwargs extraction"
            )
            logger.error(f"ğŸš¨ {error_msg}")
            raise ValueError(error_msg)

        # Build payload with id field for search server compatibility
        # sample_idì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ (ì˜ˆ: "train_14" -> "14")
        numeric_id = sample_id.split("_")[-1] if sample_id else None
        payload = [{"query": query, "request_idx": 0, "id": numeric_id}]

        print(f"\nğŸš€ [SearchTool Request] ID: {sample_id} -> {numeric_id} | Query: {query}", flush=True) # [DEBUG] ìš”ì²­ ë‚´ìš© í™•ì¸

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(self.url, json=payload, timeout=self.timeout) as resp:
                    if resp.status != 200:
                        error_text = await resp.text()
                        logger.error(f"Search API Error: {resp.status} - {error_text}")
                        return ToolResponse(text=f"Error: {resp.status}")

                    results = await resp.json()
                    # â­ï¸ [DEBUG] ì„œì¹˜ì—”ì§„ ì‘ë‹µ ì›ë³¸ í™•ì¸ (ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„)
                    # ì„œë²„ê°€ ì‹¤ì œë¡œ ë­˜ ì¤¬ëŠ”ì§€ ëˆˆìœ¼ë¡œ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.
                    print(f"ğŸ“¥ [SearchTool Raw Response]:\n{json.dumps(results, indent=2, ensure_ascii=False)}", flush=True)

                    text_content = "No search results found."
                    images_found = []
                    image_paths_found = []

                    if isinstance(results, list) and len(results) > 0:
                        search_result = results[0]
                        text_content = self._format_results(search_result)

                        if isinstance(search_result, dict) and 'results' in search_result:
                            # ê¸°ì¡´ì— ë¡œë“œëœ ì´ë¯¸ì§€ ê²½ë¡œë“¤ ìˆ˜ì§‘ (ì¤‘ë³µ ì²´í¬ìš©)
                            existing_image_paths = set()
                            if agent_data:
                                # agent_data.extra_fieldsì— ì €ì¥ëœ ì´ë¯¸ì§€ ê²½ë¡œë“¤
                                existing_paths = agent_data.extra_fields.get('image_paths', [])
                                existing_image_paths = set(existing_paths)

                            for item in search_result['results']:
                                image_path = item.get('image_file')
                                if image_path:
                                    # ì„œë²„ ë°˜í™˜ ì˜ˆì‹œ: "./search_engine/corpus/img/38_15.jpg"
                                    # ëª©í‘œ: "{local_image_root}/38_15.jpg"

                                    # (1) ì•ì˜ ./ ì œê±°
                                    clean_path = image_path.lstrip("./")

                                    # (2) search_engine/corpus/img/ ë¶€ë¶„ ì´í›„ì˜ íŒŒì¼ëª…ë§Œ ì¶”ì¶œ
                                    if "corpus/img/" in clean_path:
                                        relative_part = clean_path.split("corpus/img/", 1)[1]
                                        final_path = os.path.join(self.local_image_root, relative_part)
                                    else:
                                        final_path = os.path.join(self.local_image_root, os.path.basename(clean_path))

                                    # (3) ì¤‘ë³µ ì²´í¬: ì´ë¯¸ ë¡œë“œëœ ì´ë¯¸ì§€ëŠ” ê±´ë„ˆëœ€
                                    if final_path in existing_image_paths:
                                        logger.debug(f"Skipping duplicate image: {final_path}")
                                        continue

                                    # (4) íŒŒì¼ ì¡´ì¬ í™•ì¸ ë° ë¡œë”© - ì²« ë²ˆì§¸ ìœ íš¨ ì´ë¯¸ì§€ë§Œ ì„ íƒ
                                    if os.path.exists(final_path):
                                        try:
                                            img_obj = Image.open(final_path).convert("RGB")
                                            images_found.append(img_obj)
                                            image_paths_found.append(final_path)
                                            print(f"âœ… [SearchTool] Selected first non-duplicate image: {final_path}", flush=True)
                                            break  # ì²« ë²ˆì§¸ ìœ íš¨ ì´ë¯¸ì§€ë¥¼ ì°¾ìœ¼ë©´ ì¦‰ì‹œ ì¢…ë£Œ
                                        except Exception as e:
                                            logger.warning(f"Failed to load image {final_path}: {e}")


                    # agent_data.extra_fieldsì— ì´ë¯¸ì§€ ê²½ë¡œ ì •ë³´ ì €ì¥
                    if agent_data and image_paths_found:
                        if 'image_paths' not in agent_data.extra_fields:
                            agent_data.extra_fields['image_paths'] = []
                        agent_data.extra_fields['image_paths'].extend(image_paths_found)

                    return ToolResponse(
                        text=text_content,
                        image=images_found  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¼ë„ ê·¸ëŒ€ë¡œ ë°˜í™˜
                    )

        except asyncio.TimeoutError:
            logger.warning(f"Search API Timeout for query: {query}")
            return ToolResponse(text="Error: Search request timed out.") # 1ê°œ ê°’ ë°˜í™˜
            
        except Exception as e:
            logger.error(f"Search Tool Exception: {e}", exc_info=True)
            return ToolResponse(text=f"Error: An unexpected error occurred. {str(e)}") # 1ê°œ ê°’ ë°˜í™˜

    def _format_results(self, result_data: Any) -> str:
        # (ê¸°ì¡´ í¬ë§·íŒ… ë¡œì§ ìœ ì§€)
        if isinstance(result_data, dict) and 'results' in result_data:
            result_list = result_data['results']
            snippets = []
            for idx, item in enumerate(result_list[:self.k]):
                title = item.get("title", "No Title")
                content = item.get("text", item.get("snippet", str(item)))
                snippets.append(f"[{idx+1}] Title: {title}\nContent: {content}")
            return "\n\n".join(snippets)
        return str(result_data)